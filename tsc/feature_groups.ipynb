{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Series Characteristics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features\n",
    "\n",
    "A good overview: https://cran.r-project.org/web/packages/tsfeatures/vignettes/tsfeatures.html\n",
    "\n",
    "## Datasets\n",
    "\n",
    "List of public time series datasets: https://github.com/awesomedata/awesome-public-datasets#timeseries\n",
    "\n",
    "- Big https://www.cs.ucr.edu/~eamonn/time_series_data_2018/\n",
    "- Small https://github.com/FinYang/tsdl\n",
    "- Seasonal https://www.mcompetitions.unic.ac.cy/the-dataset/\n",
    "\n",
    "Don't forget to cite the UCR archive:\n",
    "\n",
    "    @misc{UCRArchive2018,\n",
    "        title = {The UCR Time Series Classification Archive},\n",
    "        author = {Dau, Hoang Anh and Keogh, Eamonn and Kamgar, Kaveh and Yeh, Chin-Chia Michael and Zhu, Yan \n",
    "                  and Gharghabi, Shaghayegh and Ratanamahatana, Chotirat Ann and Yanping and Hu, Bing \n",
    "                  and Begum, Nurjahan and Bagnall, Anthony and Mueen, Abdullah and Batista, Gustavo},\n",
    "        year = {2018},\n",
    "        month = {October},\n",
    "        note = {\\url{https://www.cs.ucr.edu/~eamonn/time_series_data_2018/}}\n",
    "    }\n",
    "    \n",
    "## Legend 🟩🟨🟥\n",
    "\n",
    "- 🟩 Feature is simple and easy to compute\n",
    "- 🟨 Feature is only relevant for unscaled, raw data (otherwise, it is constant and meaningless)\n",
    "- 🟥 Feature is rather complex to compute (either in terms of the formula or in terms of computational resources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from time_series_characteristics import TimeSeriesCharacteristics, NormalizedTimeSeriesCharacteristics\n",
    "\n",
    "import util"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input\n",
    "Specify the following two variables:\n",
    "- `UCR_ARCHIVE`: The path to the UCR archive main directory (contains the dataset folders).\n",
    "- `UCR_DATASET`: The name of the UCR dataset to load. Per default, the TEST-dataset will be loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UCR_ARCHIVE = INSERT_PATH_TO_UCR_MAIN_DIR_HERE\n",
    "UCR_DATASET = \"Rock\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_ucr_file(path):\n",
    "    \"\"\"Read the UCR file from the specified path into a DataFrame with a multi-index ('entity', 'time') and a single column 'value'\"\"\"\n",
    "    df = pd.read_csv(path, sep=\"\\t\", header=None, dtype=np.float64)\n",
    "    entity_index = [f\"{os.path.split(path)[1]}_{i}\" for i in df.index]\n",
    "    df.index = pd.MultiIndex.from_tuples([(i,) for i in entity_index], names=[\"entity\"])\n",
    "    x, y = df.iloc[:, 1:].copy(), df.iloc[:, 0].astype(np.int32, copy=False)\n",
    "    x[:] = preprocessing.scale(x, axis=1)\n",
    "    \n",
    "    # add entity \"index\" which is just a counter for ucr\n",
    "    x[\"entity\"] = entity_index\n",
    "\n",
    "    # un-pivot the table and use the column-wise timestamps as an index instead\n",
    "    x = pd.melt(x, id_vars=[\"entity\"], value_name=\"value\", var_name=\"time\")\n",
    "\n",
    "    # transform int-based time values to datetime (-1 because pandas labelled the first data column with 1 instead of 0)\n",
    "    x[\"time\"] = pd.to_datetime(x[\"time\"] - 1, unit=\"m\")\n",
    "\n",
    "    # set new multi-index\n",
    "    x.set_index([\"entity\", \"time\"], inplace=True, verify_integrity=True)\n",
    "    x.sort_index(inplace=True)\n",
    "    \n",
    "    return x, y\n",
    "\n",
    "def calculate_min_max_df(standardized_df, features_to_normalize, quantiles, index_levels, column_value, verbose):\n",
    "    \"\"\"Given a standardized DataFrame, calculate the quantiles for the specified features\"\"\"\n",
    "    features_to_normalize_df = util.get_feature_combination_df(\n",
    "        features_to_normalize,\n",
    "        standardized_df,\n",
    "        index_levels=index_levels,\n",
    "        column_value=column_value,\n",
    "        verbose=verbose)\n",
    "    min_max_df = features_to_normalize_df.quantile(quantiles).T\n",
    "    min_max_df.columns = [str(c) for c in min_max_df.columns]\n",
    "    return min_max_df\n",
    "\n",
    "def random_series(dtindex=False, plot=True, ax=None, random_state=None):\n",
    "    \"\"\"Get a random time series from the archive\"\"\"\n",
    "    rnd = np.random.RandomState(random_state)\n",
    "    random_entity = rnd.choice(DATA.index.get_level_values(\"entity\"))\n",
    "    sample = DATA.loc[random_entity][\"value\"].values\n",
    "    if plot:\n",
    "        noaxis = ax is None\n",
    "        if noaxis:\n",
    "            plt.figure(figsize=(4, 1))\n",
    "            ax = plt.gca()\n",
    "        ax.plot(sample)\n",
    "        if noaxis:\n",
    "            plt.show()\n",
    "        \n",
    "    if dtindex:\n",
    "        sample = pd.Series(sample, index=pd.to_datetime(list(range(len(sample))), unit=\"m\"), name=\"value\").asfreq(\"1min\")\n",
    "    return sample\n",
    "\n",
    "def apply(fun, nplots=10, extra=None, dtindex=False, **kwargs):\n",
    "    \"\"\"Apply function fun on a random sample of 10 time series and plot them along with their statistics\"\"\"\n",
    "    _, axs = plt.subplots(1, nplots, figsize=(30, 1))\n",
    "    for i, ax in enumerate(axs):\n",
    "        ts = random_series(ax=ax, dtindex=dtindex, random_state=i)\n",
    "        val = fun(ts, **kwargs)\n",
    "        if extra is not None:\n",
    "            val = extra(val)\n",
    "        ax.title.set_text(\"{:.5G}\".format(val) if not isinstance(val, str) else val)\n",
    "        \n",
    "def list_format(e):\n",
    "    \"\"\"Can be used to format list-like return values in apply\"\"\"\n",
    "    return \"[\" + \", \".join([\"{:.5G}\".format(f) for f in e]) + \"]\"\n",
    "\n",
    "def list_format_tuple(e):\n",
    "    \"\"\"Can be used to format list-like return values in apply\"\"\"\n",
    "    return \"[\" + \", \".join([\"{:.5G}\".format(f[1]) for f in e]) + \"]\"\n",
    "\n",
    "def list_format_dict_values(e):\n",
    "    \"\"\"Can be used to format list-like return values in apply\"\"\"\n",
    "    return \"[\" + \", \".join([\"{:.5G}\".format(f) for f in e.values()]) + \"]\"\n",
    "\n",
    "DATA = pd.read_csv(os.path.join(UCR_ARCHIVE, UCR_DATASET, f\"{UCR_DATASET}_TEST.tsv\"), sep=\"\\t\", header=None, index_col=0)\n",
    "DATA, LABELS = read_ucr_file(os.path.join(UCR_ARCHIVE, UCR_DATASET, f\"{UCR_DATASET}_TEST.tsv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization\n",
    "Specify which normalized features and which parameters you want to investigate. This step is only required if normalized features are required (via `NormalizedTimeSeriesCharacteristics`).\n",
    "- `params`: The parameters of the features you want to normalize for investigation.\n",
    "- `features_to_normalize`: The features (including the above parameters) you want to normalize for investigation.\n",
    "- `funcs_to_merge`: The group of parameterized features that should be normalized in bulk, rather than individually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsc = TimeSeriesCharacteristics()\n",
    "\n",
    "params = dict(\n",
    "    block_sizes=[30, 60],\n",
    "    quantiles=[0.25, 0.5, 0.75],\n",
    "    periods=[\"1h\", \"2h\"],\n",
    "    periodogram_agg_funcs=[\"min\", (\"quantile\", dict(q=0.25)), \"median\", (\"quantile\", dict(q=0.75)), \"max\"],\n",
    "    lags=[1, 2, 3]\n",
    ")\n",
    "features_to_normalize = [\n",
    "    # distributional features\n",
    "    tsc.kurtosis,\n",
    "    tsc.skewness,\n",
    "    tsc.shift,\n",
    "    *[(tsc.lumpiness, dict(block_size=b)) for b in params[\"block_sizes\"]],\n",
    "    *[(tsc.quantile, dict(q=q)) for q in params[\"quantiles\"]],\n",
    "    tsc.ratio_large_standard_deviation,\n",
    "\n",
    "    # temporal features\n",
    "    tsc.mean_second_derivative_central,\n",
    "    *[(tsc.level_shift, dict(block_size=b)) for b in params[\"block_sizes\"]],\n",
    "    *[(tsc.variance_change, dict(block_size=b)) for b in params[\"block_sizes\"]],\n",
    "    (tsc.periodicity, dict(dt_min=1.0, periods=params[\"periods\"])),\n",
    "    (tsc.agg_periodogram, dict(dt_min=1.0, funcs=params[\"periodogram_agg_funcs\"])),\n",
    "    *[(tsc.time_reversal_asymmetry_statistic, dict(lag=lag)) for lag in params[\"lags\"]],\n",
    "    tsc.linear_trend_slope,\n",
    "    (tsc.agg_linear_trend_slope, dict(block_sizes=params[\"block_sizes\"])),\n",
    "    *[(tsc.c3, dict(lag=lag)) for lag in params[\"lags\"]],\n",
    "\n",
    "    # complexity features\n",
    "    *[(tsc.kullback_leibler_score, dict(block_size=b)) for b in params[\"block_sizes\"]],\n",
    "    tsc.cid_ce\n",
    "]\n",
    "funcs_to_merge = [\n",
    "    \"quantile\",\n",
    "    \"periodicity\",\n",
    "    \"agg_periodogram\",\n",
    "    \"time_reversal_asymmetry_statistic\",\n",
    "    \"c3\"\n",
    "]\n",
    "\n",
    "ntsc = NormalizedTimeSeriesCharacteristics()\n",
    "min_max_df = calculate_min_max_df(DATA, features_to_normalize, quantiles=[0.05, 0.95], index_levels=[\"entity\"], column_value=\"value\", verbose=True)\n",
    "ntsc.init(features=features_to_normalize, min_max_df=min_max_df, funcs_to_merge=funcs_to_merge, column_min=\"0.05\", column_max=\"0.95\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 1. Distributional Features\n",
    "\n",
    "Distributional features do not depend upon the temporal structure of the data, i.e., the view the time series as a set of unordered values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measures of Dispersion\n",
    "\n",
    "- 🟩 _(Excess) Kurtosis_ is a measure of tailedness - https://en.wikipedia.org/wiki/Kurtosis\n",
    "    - Range: $[-3,~\\inf]$ because 3 is substracted from the result of the formula (Fisher’s definition)\n",
    "- 🟩 _Skewness_ is a measure of asymmetry - https://en.wikipedia.org/wiki/Skewness\n",
    "    - Range: $[-\\inf,~\\inf]$\n",
    "- 🟥 _Shift_ is the mean minus the median of those values that are smaller than the mean\n",
    "    - Range: $[-\\inf,~\\inf]$\n",
    "    - invented by Zliobaite et al. http://dx.doi.org/10.1016/j.eswa.2011.07.078"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apply(tsc.kurtosis)\n",
    "apply(ntsc.kurtosis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apply(tsc.skewness)\n",
    "apply(ntsc.skewness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apply(tsc.shift)\n",
    "apply(ntsc.shift)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measures of block-wise Dispersion\n",
    "\n",
    "- 🟥 _Lumpiness_ is the variance of the variances - https://cran.r-project.org/web/packages/tsfeatures/vignettes/tsfeatures.html#lumpiness_stability\n",
    "    - Range: $[0,~\\inf]$\n",
    "    - invented by Hyndman et al. https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7395871\n",
    "- 🟥 _Stability_ is the variance of the mean of blocks - https://cran.r-project.org/web/packages/tsfeatures/vignettes/tsfeatures.html#lumpiness_stability\n",
    "    - Range: $[0,~\\sigma^2]$ (if z-normalized, this is $[0,~1]$)\n",
    "    - invented by Hyndman et al. https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7395871"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apply(tsc.lumpiness, block_size=params[\"block_sizes\"][0])\n",
    "apply(ntsc.lumpiness, block_size=params[\"block_sizes\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apply(tsc.stability, block_size=params[\"block_sizes\"][0])\n",
    "apply(ntsc.stability, block_size=params[\"block_sizes\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measures on the Number of Duplicates\n",
    "\n",
    "- 🟩 *Normalized number of maxima duplicates* indicates the percentage of duplicate values that have the maximum value of the data\n",
    "    - Range: $[0,~1]$\n",
    "- 🟩 *Normalized number of minima duplicates* indicates the percentage of duplicate values that have the minimum value of the data\n",
    "    - Range: $[0,~1]$\n",
    "- 🟩 *Percentage of reoccurring datapoints to all datapoints* `len(different values occurring more than once) / len(different values)`\n",
    "    - Range: $[0,~1]$\n",
    "- 🟩 *Percentage of reoccurring values to all values* `# of data points occurring more than once / # values`\n",
    "    - Range: $[0,~1]$\n",
    "- 🟩 *Percentage of unique values* quantifies the number of unique values over the time series length `# unique values / # values`\n",
    "    - Range: $[0,~1]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apply(tsc.normalized_duplicates_max)\n",
    "apply(ntsc.normalized_duplicates_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apply(tsc.normalized_duplicates_min)\n",
    "apply(ntsc.normalized_duplicates_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apply(tsc.percentage_of_reoccurring_datapoints)\n",
    "apply(ntsc.percentage_of_reoccurring_datapoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apply(tsc.percentage_of_reoccurring_values)\n",
    "apply(ntsc.percentage_of_reoccurring_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apply(tsc.percentage_of_unique_values)\n",
    "apply(ntsc.percentage_of_unique_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measures on the Distribution\n",
    "\n",
    "- 🟩 _Quantiles_ indicate the threshold under which x % of the ordered values of the series are and give a hint on the distribution - https://en.wikipedia.org/wiki/Quantile\n",
    "    - Range: $[\\min(X),~\\max(X)]$\n",
    "- 🟥 _Ratio beyond r sigma_ quantifies the ratio of values that are more than a factor $r \\cdot \\sigma$ away from the mean - https://tsfresh.readthedocs.io/en/latest/api/tsfresh.feature_extraction.html#tsfresh.feature_extraction.feature_calculators.ratio_beyond_r_sigma\n",
    "    - Range: $[0,~1]$\n",
    "- 🟩 _Ratio of large Standard Deviation_ quantifies the ratio between the standard deviation and the $(\\max - \\min)$ range of the data, based on the range rule of thumb - https://www.statisticshowto.datasciencecentral.com/range-rule-of-thumb/\n",
    "    - Range: $[0,~\\inf]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for q in params[\"quantiles\"]:\n",
    "    print(f\"quantile q = {q:.2f}\")\n",
    "    apply(tsc.quantile, q=q)\n",
    "    apply(ntsc.quantile, q=q)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for r in np.arange(1, 4):\n",
    "    print(f\"ratio beyond r = {r:.2f} x sigma\")\n",
    "    apply(tsc.ratio_beyond_r_sigma, r=r)\n",
    "    apply(ntsc.ratio_beyond_r_sigma, r=r)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apply(tsc.ratio_large_standard_deviation)\n",
    "apply(ntsc.ratio_large_standard_deviation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 2. Temporal Features\n",
    "\n",
    "Temporal features take into account the temporal dependency of data points, i.e., they observe the frequency spectrum, seasonalitites, correlations with the time axis, ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measures of Temporal Dispersion\n",
    "\n",
    "- 🟩 _Mean Absolute Change_ is the average absolute difference of two consecutive values - https://tsfresh.readthedocs.io/en/latest/api/tsfresh.feature_extraction.html#tsfresh.feature_extraction.feature_calculators.mean_abs_change\n",
    "    - Range: $[0,~\\inf]$ (if z-normalized, this is approx. $[0,~2\\cdot\\sigma]$ and exactly $\\left[ 0,~2 \\cdot \\sqrt{\\dfrac{m^2 + m + \\frac{1}{4}}{m^2 +m}}~\\right]$ with $m = \\left\\lfloor\\frac{N}{2}\\right\\rfloor$ where $N$ is the number of data points)\n",
    "- 🟩 _Mean Second Derivate Central_ measures the rate of the rate of change - https://en.wikipedia.org/wiki/Second_derivative\n",
    "    - Range: $[-\\inf,~\\inf]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apply(tsc.mean_abs_change)\n",
    "apply(ntsc.mean_abs_change)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apply(tsc.mean_second_derivative_central)\n",
    "apply(ntsc.mean_second_derivative_central)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measures of block-wise Temporal Dispersion\n",
    "\n",
    "- 🟥 _Level Shift_ is the maximum difference in mean between consecutive blocks (related: _Step Detection_ - https://en.wikipedia.org/wiki/Step_detection)\n",
    "    - Range: $[0,~\\inf]$\n",
    "    - invented by Hyndman et al. https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7395871\n",
    "- 🟥 _Variance Change_ is the maximum difference in variance between consecutive blocks\n",
    "    - Range: $[0,~\\inf]$\n",
    "    - invented by Hyndman et al. https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7395871"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apply(tsc.level_shift, block_size=params[\"block_sizes\"][0])\n",
    "apply(ntsc.level_shift, block_size=params[\"block_sizes\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apply(tsc.variance_change, block_size=params[\"block_sizes\"][0])\n",
    "apply(ntsc.variance_change, block_size=params[\"block_sizes\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measures of Temporal Similarity\n",
    "\n",
    "- 🟥 _Hurst_ is a measure of long-term memory of a time series, related to auto-correlation (related: _Detrended Fluctuation Analysis DFA_) - https://en.wikipedia.org/wiki/Hurst_exponent\n",
    "    - Range: $[0, 1]$ (if properly clamped)\n",
    "- 🟩 _Auto-Correlation (ACF)_ is the correlation of a signal with a lagged version of itself (related: _Correlogram_) - https://en.wikipedia.org/wiki/Autocorrelation\n",
    "    - Range: $[0, 1]$ because we normalize the original $[-1, 1]$ range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apply(tsc.hurst)\n",
    "apply(ntsc.hurst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lag in params[\"lags\"]:\n",
    "    print(f\"lag = {lag:d}\")\n",
    "    apply(tsc.autocorrelation, lag=lag)\n",
    "    apply(ntsc.autocorrelation, lag=lag)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measures in the Frequency Spectrum\n",
    "\n",
    "- 🟥 _Periodicity_ identifies the power (intensity) of various frequencies in the signal (related: _Periodogram_) - https://en.wikipedia.org/wiki/Periodogram\n",
    "    - Range: $[0, \\inf]$\n",
    "- 🟥 _Aggregated Periodogram_ computes the periodogram and returns the results of user-defined aggregation functions upon this periodogram (e.g. fivenum) - https://en.wikipedia.org/wiki/Periodogram\n",
    "    - Range: $[0,~\\inf]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for period in params[\"periods\"]:\n",
    "    print(f\"periodicity {period}\")\n",
    "    apply(tsc.periodicity, dtindex=True, extra=lambda e: e[0], periods=[period])\n",
    "    apply(ntsc.periodicity, dtindex=True, extra=lambda e: e[0][1], periods=[period])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apply(tsc.agg_periodogram, funcs=params[\"periodogram_agg_funcs\"], nplots=4, dtindex=True, extra=list_format_tuple)\n",
    "apply(ntsc.agg_periodogram, funcs=params[\"periodogram_agg_funcs\"], nplots=4, dtindex=True, extra=list_format_tuple)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measures of Linearity and Trends\n",
    "\n",
    "- 🟩 _Linear Trend_ is a measure of linearity (Slope and R squared) - https://en.wikipedia.org/wiki/Coefficient_of_determination\n",
    "    - Range: Slope $[-\\inf,~\\inf]$ and R squared $[0,~1]$\n",
    "- 🟩 _Aggregated Linear Trend_ (Slope and R squared) - https://en.wikipedia.org/wiki/Coefficient_of_determination\n",
    "    - Range: Variance of Slopes $[-\\inf,~\\inf]$ and Mean of R squared $[0,~1]$\n",
    "- 🟥 _C3 Score_ is a measure of non-linearity from the physics domain - https://journals.aps.org/pre/abstract/10.1103/PhysRevE.55.5443\n",
    "    - Range: $[-\\inf,~\\inf]$\n",
    "    - invented by Schreiber et al. https://journals.aps.org/pre/abstract/10.1103/PhysRevE.55.5443\n",
    "- 🟩 _Time-reversal Asymmetry Statistic_ measures the asymmetry of the time series if reversed, which can be a measure of non-linearity - https://www.pks.mpg.de/~tisean/TISEAN_2.1/docs/docs_f/timerev.html\n",
    "    - Range: $[-\\inf,~\\inf]$\n",
    "    - used by https://ieeexplore.ieee.org/document/6786425 with $\\text{lag}=3$ (page 3030)\n",
    "    - invented by https://arxiv.org/pdf/chao-dyn/9909037.pdf (page 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"slope\")\n",
    "apply(tsc.linear_trend_slope)\n",
    "apply(ntsc.linear_trend_slope)\n",
    "plt.show()\n",
    "\n",
    "print(\"r^2\")\n",
    "apply(tsc.linear_trend_rvalue2)\n",
    "apply(ntsc.linear_trend_rvalue2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"variance of slopes with chunk size = {}\".format(params[\"block_sizes\"]))\n",
    "apply(tsc.agg_linear_trend_slope, block_sizes=params[\"block_sizes\"], extra=list_format_tuple)\n",
    "apply(ntsc.agg_linear_trend_slope, block_sizes=params[\"block_sizes\"], extra=list_format_tuple)\n",
    "plt.show()\n",
    "\n",
    "print(\"mean of r^2 values with chunk size = {}\".format(params[\"block_sizes\"]))\n",
    "apply(tsc.agg_linear_trend_rvalue2, block_sizes=params[\"block_sizes\"], extra=list_format_tuple)\n",
    "apply(ntsc.agg_linear_trend_rvalue2, block_sizes=params[\"block_sizes\"], extra=list_format_tuple)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lag in params[\"lags\"]:\n",
    "    print(f\"lag = {lag:d}\")\n",
    "    apply(tsc.c3, lag=lag)\n",
    "    apply(ntsc.c3, lag=lag)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lag in params[\"lags\"]:\n",
    "    print(f\"lag = {lag:d}\")\n",
    "    apply(tsc.time_reversal_asymmetry_statistic, lag=lag)\n",
    "    apply(ntsc.time_reversal_asymmetry_statistic, lag=lag)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 3. Complexity Features\n",
    "\n",
    "Complexity Features measure the \"randomness\" of a time series, its entropy, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measures of Entropy\n",
    "\n",
    "- 🟩 _Binned Entropy_ (related: _Approximate Entropy_, _Sample Entropy_) - https://en.wikipedia.org/wiki/Sample_entropy - https://en.wikipedia.org/wiki/Approximate_entropy\n",
    "    - Range: $[0,~1]$ because we normalize the original $[0,~\\log(n_{\\text{bins}})]$ range\n",
    "- 🟥 _Kullback–Leibler score_ is the maximum difference in the KL score between consecutive blocks, where the KL score measures the probabilistic difference between two probability distributions (related: _Total Variation Distance_, _Rényi Divergences_) - https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence\n",
    "    - Range: $[-\\inf,~\\inf]$\n",
    "    - invented by Hyndman et al. https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7395871\n",
    "- 🟥 _Index of Maximum Kullback-Leibler Score_ is the relative location where the maximum KL Score was found\n",
    "    - Range: $[0,~1]$\n",
    "    - invented by Hyndman et al. https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7395871"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apply(tsc.binned_entropy, max_bins=10)\n",
    "apply(ntsc.binned_entropy, max_bins=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apply(tsc.kullback_leibler_score, block_size=params[\"block_sizes\"][0])\n",
    "apply(ntsc.kullback_leibler_score, block_size=params[\"block_sizes\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apply(tsc.index_of_kullback_leibler_score, block_size=params[\"block_sizes\"][0])\n",
    "apply(tsc.index_of_kullback_leibler_score, block_size=params[\"block_sizes\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measures of (miscellaneous) Complexity and Permutation\n",
    "\n",
    "- 🟩 _CID Score_ is a measure of complexity invariance - https://link.springer.com/article/10.1007/s10618-013-0312-3\n",
    "    - Range: $[0,~\\inf]$\n",
    "    - invented by Batista et al. https://link.springer.com/article/10.1007/s10618-013-0312-3\n",
    "- 🟥 _Permutation Analysis_ is a measure of complexity through permutation to show the presence of temporal patterns\n",
    "    - Range: score $[0,~1]$\n",
    "- 🟥 _Swinging Door Compression Rate_ compresses the signal under a given bound $\\epsilon$ (specified in the value range of the signal, higher = more compression) - https://github.com/gfoidl/DataCompression/blob/master/doc/SwingingDoor.md\n",
    "    - Range: $[0,~1]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apply(tsc.cid_ce)\n",
    "apply(ntsc.cid_ce)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# not implemented due to NDA\n",
    "# apply(tsc.permutation_analysis)\n",
    "# apply(ntsc.permutation_analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apply(tsc.swinging_door_compression_rate, eps=0.1)\n",
    "apply(ntsc.swinging_door_compression_rate, eps=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measures of Flatness\n",
    "\n",
    "- 🟩 _Normalized Number of Crossing Points_ is the number (or, percentage) of times a time series crosses the mean line (related: _Fickleness_ by Matijas et al. https://www.sciencedirect.com/science/article/pii/S095741741300078X)\n",
    "    - Range: $[0,~1]$\n",
    "- 🟩 _Normalized Count above Mean_ is the percentage of values that is higher than the mean\n",
    "    - Range: $[0, 1]$\n",
    "- 🟩 _Normalized Count below Mean_ is the percentage of values that is lower than the mean\n",
    "    - Range: $[0, 1]$\n",
    "- 🟩 _Normalized Longest Strike above Mean_ is the relative length of the longest series of consecutive data points above the mean\n",
    "    - Range: $[0, 1]$\n",
    "- 🟩 _Normalized Longest Strike below Mean_ is the relative length of the longest series of consecutive data points below the mean\n",
    "    - Range: $[0, 1]$\n",
    "- 🟥 _Flat Spots_ measures the maximum run-length of values, if they are divided into bins (either linear, or quantile-based) - https://ieeexplore.ieee.org/abstract/document/7395871\n",
    "    - Range: $[0, 1]$ per interval\n",
    "    - invented by Hyndman et al. https://ieeexplore.ieee.org/abstract/document/7395871"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apply(tsc.normalized_crossing_points)\n",
    "apply(ntsc.normalized_crossing_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apply(tsc.normalized_above_mean)\n",
    "apply(ntsc.normalized_above_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apply(tsc.normalized_below_mean)\n",
    "apply(ntsc.normalized_below_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apply(tsc.normalized_longest_strike_above_mean)\n",
    "apply(ntsc.normalized_longest_strike_above_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apply(tsc.normalized_longest_strike_below_mean)\n",
    "apply(ntsc.normalized_longest_strike_below_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apply(tsc.flat_spots, nplots=2, extra=list_format_dict_values)\n",
    "apply(ntsc.flat_spots, nplots=2, extra=list_format_dict_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measures of Peaks/Peakiness\n",
    "\n",
    "- 🟩 _Normalized Number of Peaks_ counts how many values are bigger than their (supporting) neighbours\n",
    "    - Range: $[0, 1]$\n",
    "- 🟥 _Normalized Step Changes_ indicates how often the time series significantly shifts its value range - https://www.sciencedirect.com/science/article/pii/S0925231210001074\n",
    "    - Range: $[0, 1]$\n",
    "    - invented by Lemke et al. https://www.sciencedirect.com/science/article/pii/S0925231210001074"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apply(tsc.normalized_number_peaks, n=5)\n",
    "apply(ntsc.normalized_number_peaks, n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apply(tsc.step_changes, window_len=60)\n",
    "apply(ntsc.step_changes, window_len=60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 4. Statistical Tests\n",
    "\n",
    "Tests on stationarity, unit roots, etc. - https://www.statisticshowto.datasciencecentral.com/stationarity/ - https://www.statisticshowto.datasciencecentral.com/unit-root/ - https://arch.readthedocs.io/en/latest/unitroot/tests.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stationarity and Unit Roots\n",
    "\n",
    "- 🟩 _Augmented Dickey Fuller (ADF) Test_ - https://en.wikipedia.org/wiki/Augmented_Dickey%E2%80%93Fuller_test\n",
    "    - Range: p-value $[0, 1]$\n",
    "- 🟩 _Kwiatkowski–Phillips–Schmidt–Shin (KPSS) Test_ - https://en.wikipedia.org/wiki/KPSS_test\n",
    "    - Range: p-value $[0, 1]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apply(tsc.adf)\n",
    "apply(ntsc.adf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apply(tsc.kpss)\n",
    "apply(ntsc.kpss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Dropped Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### tsfresh: `abs_energy()`\n",
    "- 🟨 _Absolute Energy_ is the area under the squared magnitude of the series - https://en.wikipedia.org/wiki/Energy_(signal_processing)\n",
    "    - Group: Measures of Energy\n",
    "    - Range: $[0,~\\inf]$ (if z-normalized, this is always $n$ ⚠️ **do not use on z-normalized data**)\n",
    "- we have z-normalized data so we cannot use this\n",
    "\n",
    "##### tsfresh: `standard_deviation()`\n",
    "- 🟨 _Standard Deviation_ is a measure of variance (related: _Variance_) - https://en.wikipedia.org/wiki/Standard_deviation\n",
    "    - Group: Measures of Dispersion\n",
    "    - Range: $[-\\inf,~\\inf]$ (if z-normalized, this is always $1$ ⚠️ **do not use on z-normalized data**)\n",
    "- we have z-normalized data so we cannot use this\n",
    "\n",
    "##### tsfresh: `partial_autocorrelation(...)`\n",
    "- 🟥 _Partial Auto-Correlation (PACF)_ is the same as ACF but with a \"removal\" of the correlation of smaller lags - https://en.wikipedia.org/wiki/Partial_autocorrelation_function\n",
    "    - Group: Measures of Temporal Self-Similarity\n",
    "    - Range: $[0, 1]$ because we normalize the original $[-1, 1]$ range\n",
    "- Very expensive + the implementation yields unreliable outputs often outside the expected data range of $[-1, 1]$\n",
    "\n",
    "##### tsfresh: `ar_coefficient(...)`\n",
    "- 🟥 _AR (Auto-Regression) Coefficient_ - https://en.wikipedia.org/wiki/Autoregressive_model\n",
    "    - Group: Measures of Linearity and Trends\n",
    "    - Range: $[-\\inf,~\\inf]$\n",
    "- Too complex. We do not want an entire complex model fitted to retrieve a feature.\n",
    "    \n",
    "##### tsfresh: `change_quantiles()`\n",
    "- Pre-processing, no feature.\n",
    "\n",
    "##### tsfresh: `first_location_of_maximum()` `first_location_of_minimum()` `last_location_of_maximum()` `last_location_of_minimum()`\n",
    "- The exact location of maxima and minima is not relevant in our domain.\n",
    "\n",
    "##### tsfresh: `friedrich_coefficients()` `max_langevin_fixed_point()`\n",
    "- https://en.wikipedia.org/wiki/Langevin_equation\n",
    "- We don't want to fit a parameterized model (too complex).\n",
    "\n",
    "##### tsfresh: `index_mass_quantile()` `quantile()`\n",
    "- Pre-processing, no feature.\n",
    "\n",
    "##### tsfresh: `range_count()` `value_count()`\n",
    "- Helper function, no feature.\n",
    "\n",
    "##### tsfresh: `large_standard_deviation()`\n",
    "- Boolean feature, use a ratio instead.\n",
    "\n",
    "##### tsfresh: `length()`\n",
    "- All our time series have the same length.\n",
    "\n",
    "##### tsfresh: `number_crossing_m()`\n",
    "- Helper function, no feature.\n",
    "\n",
    "##### tsfresh: `sum_of_reoccurring_data_points()` `sum_of_reoccurring_values()` `sum_values()`\n",
    "- Our flat spot measures and duplicate measures already cover these in a more general way.\n",
    "\n",
    "##### tsfresh: `num_duplicates()` `num_duplicates_max()` `num_duplicates_min()`\n",
    "- We use the normalized variants of these.\n",
    "\n",
    "##### tsfresh: `has_duplicate()` `has_duplicate_max()` `has_duplicate_min()`\n",
    "- Boolean features, use a ratio instead.\n",
    "\n",
    "##### tsfresh: `variance_larger_than_standard_deviation()`\n",
    "- Boolean features, use a ratio instead.\n",
    "\n",
    "##### tsfresh: `mean()` `median()` `maximum()`  `minimum()`\n",
    "- Not useful on scaled data\n",
    "\n",
    "##### tsfresh: `mean_change()`\n",
    "- Compute `(a[-1] - a[0]) / (len(a) - 1)`.\n",
    "- Not meaningful, already covered by trend measures.\n",
    "\n",
    "##### tsfresh: `absolute_sum_of_changes()`\n",
    "- This is the same as `mean_abs_change()`, but unnormalized\n",
    "\n",
    "##### tsfresh: `agg_autocorrelation(x, param)`\n",
    "- Computationally too expensive.\n",
    "\n",
    "##### tsfresh: `spkt_welch_density()` `fft_coefficient()`\n",
    "- Already covered by periodicity metrics\n",
    "\n",
    "##### tsfresh: `linear_trend_timewise()`\n",
    "- Only relevent if our time series would have gaps or would not be evenly spaced\n",
    "\n",
    "##### tsfresh: `approximate_entropy(x, m, r)` / `sample_entropy(x)`\n",
    "- Computationally too expensive. We use `binned_entropy(x, max_bins)` as an alternative.\n",
    "\n",
    "##### tsfresh: `number_cwt_peaks()`\n",
    "- CWT Peaks smooths the signal with a window and counts the remaining peaks that are higher than the SNR.\n",
    "- Computationally too expensive. We use `number_peaks(x, n)` as an alternative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Index of Disperion\n",
    "- 🟨 _Index of Dispersion_ is a measure of burstiness (related: _Fano Factor_, _Variance-to-Mean Ratio_) - https://en.wikipedia.org/wiki/Index_of_dispersion\n",
    "    - Group: measures of dispersion\n",
    "    - Range: $[-\\inf,~\\inf]$ (if z-normalized, this is always $\\inf$ ⚠️ **do not use on z-normalized data**)\n",
    "    - used by Talagala et al. http://dx.doi.org/10.1080/10618600.2019.1617160\n",
    "    - code:\n",
    "            def index_of_dispersion(data):\n",
    "                std = fc.standard_deviation(data)\n",
    "                mu = np.mean(data)\n",
    "                return std ** 2 / mu if mu != 0 else np.nan\n",
    "\n",
    "            apply(index_of_dispersion)\n",
    "- we have z-normalized data so we cannot use this\n",
    "\n",
    "##### Moving Average (MA) coefficient (already implemented: `ma_coefficients(...)`)\n",
    "- 🟥 _MA (Moving Average) Coefficient_ - https://en.wikipedia.org/wiki/Moving-average_model\n",
    "    - Group: Measures of Linearity and Trends\n",
    "    - Range: $[-\\inf,~\\inf]$\n",
    "- Too complex. We do not want an entire complex model fitted to retrieve a feature.\n",
    "\n",
    "##### Exogenous Features\n",
    "- We are only interested in the raw feature and do not incorporate domain factors or exogenous factors initially.\n",
    "\n",
    "##### Auto-Correlation of higher orders\n",
    "- Already covered by Correlogram.\n",
    "\n",
    "##### Highest Auto-Correlation\n",
    "- Already covered by Correlogram.\n",
    "\n",
    "##### Granularity\n",
    "- We assume a fixed difference in our domain (1 minute).\n",
    "\n",
    "##### Non-Linearity\n",
    "- Teräsvirta’s neural network test for nonlinearity / nonlinearity is estimated by generating surrogate time series as the realisation of the null hypothesis that the series is linear.\n",
    "- If the delay vector representations of original and surrogate series are significantly different, the time series is considered to be nonlinear\n",
    "- We don't want to fit a neural network as a feature (too complex).\n",
    "\n",
    "##### Local Variation Features\n",
    "- IQR of Lag-1-Difference and Correlation between time series and Lag-1-Difference\n",
    "- Already covered by Correlogram.\n",
    "\n",
    "##### Normalized Power of Specified Frequency\n",
    "- Already covered by Periodogram.\n",
    "\n",
    "##### Curvature\n",
    "- Already covered by distributional features.\n",
    "\n",
    "##### Variance\n",
    "- Already covered by standard deviation.\n",
    "\n",
    "##### Symmetry\n",
    "- 🟨 Computes `|mean - median| / (max - min)` - https://en.wikipedia.org/wiki/Symmetric_probability_distribution#Properties\n",
    "- Always zero for normalized data (⚠️ **do not use on z-normalized data**).\n",
    "\n",
    "##### Traversity\n",
    "- Traversity is the standard deviation of the difference between time-series `y` and `y_per` where `y_n` is the `n`-th ACF lag of `y`\n",
    "- Seems arbitrary and might already be covered by Correlogram.\n",
    "\n",
    "##### Decompositions (Peaks, Troughs, Spikiness, ETS, ARIMA, Seasonalities, Trends)\n",
    "- STL is problematic since it requires the main period for the seasonal part, which we do not know.\n",
    "- Computationally too expensive. We don't want to fit a model.\n",
    "\n",
    "##### Predictability\n",
    "- We don't want to fit a model.\n",
    "\n",
    "##### Statistical Tests (Elliott-Rothenberg-Stock, Schmidt-Philips, Philips-Perron (PP), Zivot-Andrews, Durbin-Watson)\n",
    "- We already calculate features for some of them that already quantify the result.\n",
    "- Many of them are very complicated, we only use ADF and KPSS tests.\n",
    "\n",
    "##### Dynamic Time Wraping (DTW)\n",
    "- Computationally too expensive.\n",
    "\n",
    "##### SAX Quadruple\n",
    "- We don't want parameterized compression algorithms\n",
    "\n",
    "##### Fickleness\n",
    "- Fickleness is the ratio of the number of times a time-series reverts across its mean and the length of the time-series `n` -https://www.sciencedirect.com/science/article/pii/S095741741300078X\n",
    "- We already have `normalized_crossing_points()`\n",
    "\n",
    "##### Turning Points\n",
    "- A turning point for series is given if `y_i` is a local maximum or minimum for its two closest neighbours\n",
    "- This is similar to `normalized_number_peaks(n)` with `n = 2`\n",
    "\n",
    "##### Detrended Fluctuation Analysis (DFA)\n",
    "- Computationally too expensive.\n",
    "\n",
    "##### Lyapunov (Chaos)\n",
    "- Computationally too expensive.\n",
    "\n",
    "##### Correlation Dimension\n",
    "- Computationally too expensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
